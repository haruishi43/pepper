#!/usr/bin/env python3

from collections import defaultdict
import copy
import itertools

import numpy as np

import torch
import torch.distributed as dist
from torch.utils.data import DistributedSampler, Sampler

from ..builder import SAMPLERS


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def reorder_index(batch_indices, world_size):
    """Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:
    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


@SAMPLERS.register_module()
class NaiveIdentityDistributedSampler(DistributedSampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - dataset (Dataset)
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(
        self,
        dataset,
        num_replicas=None,
        rank=None,
        seed=0,
        shuffle=True,
        batch_size=32,
        num_instances=4,
    ):
        super().__init__(
            dataset,
            num_replicas=num_replicas,
            rank=rank,
            shuffle=shuffle,
            seed=seed,
            drop_last=False,
        )
        assert not (batch_size % num_instances), \
            "batch_size should be divisible by num_instances"
        self.num_instances = num_instances
        self.num_pids_per_batch = batch_size // self.num_instances
        self.batch_size = batch_size

        # avoid having to run pipelines
        data_infos = copy.deepcopy(
            [info["sampler_info"] for info in dataset.data_infos]
        )

        self.pid_index = defaultdict(list)
        for index, info in enumerate(data_infos):
            pid = info["pid"]
            # camid = info["camid"]
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        print(self.pids)
        print(self.pid_index)

    def __iter__(self):
        available_pids = copy.deepcopy(self.pids)

        if self.shuffle:
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            pid_indices = torch.randperm(len(available_pids), generator=g).tolist()
            available_pids = [available_pids[i] for i in pid_indices]

        batch_idxs_dict = {}
        indices = []
        while len(available_pids) >= self.num_pids_per_batch:
            batch_indices = []

            selected_pids = np.random.choice(
                available_pids, self.num_pids_per_batch, replace=False
            ).tolist()
            for pid in selected_pids:
                # Register pid in batch_idxs_dict if not
                if pid not in batch_idxs_dict:
                    idxs = copy.deepcopy(self.pid_index[pid])
                    if len(idxs) < self.num_instances:
                        idxs = np.random.choice(
                            idxs, size=self.num_instances, replace=True
                        ).tolist()
                    np.random.shuffle(idxs)
                    batch_idxs_dict[pid] = idxs

                avl_idxs = batch_idxs_dict[pid]
                for _ in range(self.num_instances):
                    batch_indices.append(avl_idxs.pop(0))

                if len(avl_idxs) < self.num_instances:
                    available_pids.remove(pid)

            assert len(batch_indices) == self.batch_size
            indices += batch_indices

        indices = reorder_index(indices, self.num_replicas)
        indices = itertools.islice(indices, self.rank, None, self.num_replicas)
        return iter(indices)

    # def old_iter(self):
    #     start = self.rank
    #     yield from itertools.islice(
    #         self._infinite_indices(),
    #         start,
    #         None,  # iteration continues until the iterator is exhausted
    #         self.num_replicas,
    #     )

    # def _infinite_indices(self):

    #     # shuffle

    #     while True:
    #         avl_pids = copy.deepcopy(self.pids)
    #         batch_idxs_dict = {}

    #         batch_indices = []
    #         while len(avl_pids) >= self.num_pids_per_batch:
    #             selected_pids = np.random.choice(
    #                 avl_pids, self.num_pids_per_batch, replace=False
    #             ).tolist()
    #             for pid in selected_pids:
    #                 # Register pid in batch_idxs_dict if not
    #                 if pid not in batch_idxs_dict:
    #                     idxs = copy.deepcopy(self.pid_index[pid])
    #                     if len(idxs) < self.num_instances:
    #                         idxs = np.random.choice(
    #                             idxs, size=self.num_instances, replace=True
    #                         ).tolist()
    #                     np.random.shuffle(idxs)
    #                     batch_idxs_dict[pid] = idxs

    #                 avl_idxs = batch_idxs_dict[pid]
    #                 for _ in range(self.num_instances):
    #                     batch_indices.append(avl_idxs.pop(0))

    #                 if len(avl_idxs) < self.num_instances:
    #                     avl_pids.remove(pid)

    #             if len(batch_indices) == self.batch_size:
    #                 yield from reorder_index(batch_indices, self.num_replicas)
    #                 batch_indices = []
